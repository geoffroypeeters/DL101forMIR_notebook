{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Multi-Pitch Estimation (MPE) based on HCQT/Conv2D/U-Net\n",
    "\n",
    "- date: 2024-10-10\n",
    "- author: geoffroy.peeters@telecom-paris.fr\n",
    "\n",
    "This notebook was created for the tutorial held at ISMIR-2024 \"Deep-Learning 101 for Audio-based Music Information Retrieval\".\n",
    "It illustrates the use of various deep-learning bricks to solve the task of \"Multi-Pitch-Estimation\".\n",
    "\n",
    "\n",
    "Part of the code is based on \n",
    "- \"Deep Salience\" model: https://github.com/rabitt/ismir-2021-tutorial-case-studies/tree/main/pitch_tracking\n",
    "\n",
    "Datasets are available at\n",
    "- Bach10 https://github.com/flippy-fyp/Bach10_v1.1\n",
    "- MAPS https://adasp.telecom-paris.fr/resources/2010-07-08-maps-database/\n",
    "\n",
    "The two datasets illustrate different type of annotations.\n",
    "- Bach10 annotates the pitch as the four (one for each stem) fundamental frequencies existing a each time-frame (frame analysis)\n",
    "- MAPS annotates the pitch as the [start,end,midi-note] of each note (the polyphony varies over time) of the piano performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "In case the notebook is run on GoogleColab or others, we first need to\n",
    "- get the packages: `git clone``\n",
    "- get the datasets\n",
    "\n",
    "We also test the models on two different datasets:\n",
    "- Bach10\n",
    "- MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_deploy = False\n",
    "\n",
    "if do_deploy:\n",
    "    !git clone https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook.git\n",
    "    %cd deeplearning-101-audiomir_notebook\n",
    "    !ls\n",
    "\n",
    "    import urllib.request\n",
    "    import shutil\n",
    "    ROOT = 'https://perso.telecom-paristech.fr/gpeeters/tuto_DL101forMIR/'\n",
    "    \n",
    "    #hdf5_audio_file, pyjama_annot_file = 'bach10_audio.hdf5.zip', 'bach10.pyjama'\n",
    "    hdf5_audio_file, file_annot = 'maps_audio.hdf5.zip', 'maps.pyjama'\n",
    "    \n",
    "    urllib.request.urlretrieve(ROOT + hdf5_audio_file, hdf5_audio_file)\n",
    "    if hdf5_audio_file.endswith('.zip'): shutil.unpack_archive(hdf5_audio_file, './')\n",
    "    urllib.request.urlretrieve(ROOT + pyjama_annot_file, pyjama_annot_file)\n",
    "\n",
    "    ROOT = './'\n",
    "\n",
    "else:\n",
    "\n",
    "    ROOT = '/tsi/data_doctorants/gpeeters/_data/'\n",
    "\n",
    "#base, do_f0_annot = 'bach10', 'frame'\n",
    "base, do_f0_annot = 'maps', 'segment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "#import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torchsummary\n",
    "\n",
    "! pip install lightning --quiet\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "! pip install wandb --quiet\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "import h5py\n",
    "import pprint as pp\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import feature\n",
    "\n",
    "# -----------------------------\n",
    "import model_factory \n",
    "import importlib\n",
    "importlib.reload(model_factory) \n",
    "importlib.reload(feature) \n",
    "\n",
    "from argparse import Namespace\n",
    "! pip install munch --quiet\n",
    "from munch import munchify\n",
    "\n",
    "# -----------------------------\n",
    "! pip install mir_eval --quiet\n",
    "import mir_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set fixed seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 42\n",
    "torch.manual_seed(seed)         # For CPU\n",
    "random.seed(seed)               # For Python's built-in random module\n",
    "np.random.seed(seed)            # For NumPy\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)          # For current GPU\n",
    "    torch.cuda.manual_seed_all(seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_audio_file = f'{ROOT}/{base}_audio.hdf5'\n",
    "hdf5_feat_file = f'{ROOT}/{base}_feat.hdf5'\n",
    "pyjama_annot_file = f'{ROOT}/{base}.pyjama'\n",
    "\n",
    "param_patch = Namespace()\n",
    "param_patch.L_frame = 64\n",
    "param_patch.STEP_frame = int(param_patch.L_frame/2)\n",
    "\n",
    "param_model = Namespace()\n",
    "param_model.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "param_model.batch_size = 32\n",
    "\n",
    "param_lightning = Namespace()\n",
    "param_lightning.max_epochs = 500\n",
    "param_lightning.dirpath='./_pitch_lighning/'\n",
    "param_lightning.filename='best_model_pitch'\n",
    "\n",
    "param_wandb = Namespace()\n",
    "param_wandb.save_dir = './_pitch_wandb/'\n",
    "param_wandb.project_name = 'wandb_pitch'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loading pyjama/hdf5\n",
    "\n",
    "All the audio data of a dataset are stored in a single [.hdf5](https://docs.h5py.org/) file.\n",
    "Each `key` corresponds to an entry, a specific audiofile.\n",
    "Its array contains the audio wavform and the attribute `sr_hz` provides its sampling rate.\n",
    "\n",
    "All the annotations of a dataset are stored in a single *.pyjama file.\n",
    "As [JAMS](https://github.com/marl/jams) files, .pyjama files are JSON files.\n",
    "However, a single .pyjama file can contain the annotations of ALL entries of a dataset.\n",
    "Its specifications are described here [DOC](https://github.com/geoffroypeeters/pyjama).\n",
    "The values of the `filepath` field of the .pyjama file correspond to the `key` values of the .hdf5 file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pyjama_annot_file, encoding = \"utf-8\") as json_fid:\n",
    "    data_d = json.load(json_fid)\n",
    "entry_l = data_d['collection']['entry']\n",
    "\n",
    "if do_f0_annot == 'frame':      pp.pprint(np.asarray(entry_l[4]['f0multi'][0]['value'])[20:25,:])\n",
    "elif do_f0_annot == 'segment':  pp.pprint(entry_l[4]['pitchmidi'][:5])\n",
    "\n",
    "\n",
    "# --- get list of audio files\n",
    "audiofile_l = [entry['filepath'][0]['value'] for entry in entry_l]\n",
    "print(f'number of audio: {len(audiofile_l)}')\n",
    "# --- example of audio (for illustration)\n",
    "pp.pprint(audiofile_l[:5])\n",
    "\n",
    "\n",
    "with h5py.File(hdf5_audio_file, 'r') as hdf5_fid:\n",
    "    pp.pprint(f\"audio shape: {hdf5_fid[audiofile_l[0]][:].shape}\")\n",
    "    pp.pprint(f\"audio sample-rate: {hdf5_fid[audiofile_l[0]].attrs['sr_hz']}\")\n",
    "    #pp.pprint(hdf5_fid['/'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define features\n",
    "\n",
    "We define the feature that will be used as input to the model, the `X`, and test it on an audio file.\n",
    "We will use here either the CQT or Harmonic-CQT provided by the function `feature.f_get_hcqt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_to_f0 = lambda midi: 440*2**((midi-69)/12)\n",
    "f0_to_midi = lambda f0: np.log2(f0/440)*12+69\n",
    "flog = lambda x: np.log(1+1000*x)-np.log(1+1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_hcqt = Namespace()\n",
    "param_hcqt.tmp_dir = './_pitch_feature/'\n",
    "#param_hcqt.h_l = [1] # --- CQT\n",
    "param_hcqt.h_l = [0.5, 1, 2, 3, 4, 5] # --- HCQT\n",
    "BINS_PER_SEMITONE = 5\n",
    "N_OCTAVES = 6\n",
    "param_hcqt.BINS_PER_OCTAVE = 12 * BINS_PER_SEMITONE\n",
    "param_hcqt.N_BINS = N_OCTAVES * param_hcqt.BINS_PER_OCTAVE\n",
    "param_hcqt.FMIN = 32.7\n",
    "param_hcqt.HOP_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST\n",
    "idx_file = 0\n",
    "with h5py.File(hdf5_audio_file, 'r') as hdf5_fid:\n",
    "    audio_v = hdf5_fid[audiofile_l[idx_file]][:]\n",
    "    sr_hz = hdf5_fid[audiofile_l[idx_file]].attrs['sr_hz']\n",
    "CQT_3m, cqt_time_sec_v, cqt_frequency_hz_v = feature.f_get_hcqt(audio_v, sr_hz, param_hcqt)\n",
    "print(f'{CQT_3m.shape} {cqt_frequency_hz_v[0], cqt_frequency_hz_v[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch/parrallel processing of feature extraction\n",
    "\n",
    "Since CQT or Harmonic-CQT are costly to be computed on the fly, we will pre-compute the features for all files of the dataset.\n",
    "To speed-up the computation, we will use several CPU in parrallel `multiprocessing.Pool`, each compute the features for a given file, then store the results in a `.npz` file.\n",
    "We then collect all the `.npz` file in a single `.hdf5` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_parrallel(audio_file):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    with h5py.File(hdf5_audio_file, 'r') as audio_fid:\n",
    "        audio_value_v = audio_fid[audio_file][:]\n",
    "        audio_sr_hz = audio_fid[audio_file].attrs['sr_hz']\n",
    "        cqt_value_3m, cqt_time_sec_v, cqt_frequency_hz_v = feature.f_get_hcqt(audio_value_v, audio_sr_hz, param_hcqt)\n",
    "        if not os.path.exists(param_hcqt.tmp_dir):\n",
    "            os.makedirs(param_hcqt.tmp_dir)\n",
    "        np.savez(f\"{param_hcqt.tmp_dir}/{audio_file}.npz\", cqt_value_3m=cqt_value_3m, cqt_time_sec_v=cqt_time_sec_v, cqt_frequency_hz_v=cqt_frequency_hz_v)\n",
    "\n",
    "with h5py.File(hdf5_audio_file, 'r') as hdf5_fid:\n",
    "    audio_file_l = [audio_file for audio_file in hdf5_fid['/'].keys()]\n",
    "\n",
    "with Pool(16) as p: \n",
    "    p.map(f_parrallel, audio_file_l)\n",
    "\n",
    "with h5py.File(hdf5_feat_file, 'w') as feat_fid:\n",
    "    for audio_file in tqdm(audio_file_l):\n",
    "        data = np.load(f\"{param_hcqt.tmp_dir}/{audio_file}.npz\")\n",
    "        feat_fid['/' + audio_file + '/cqt_value_3m/'] = data['cqt_value_3m']\n",
    "        feat_fid['/' + audio_file + '/cqt_time_sec_v/'] = data['cqt_time_sec_v']\n",
    "        feat_fid['/' + audio_file + '/cqt_frequency_hz_v/'] = data['cqt_frequency_hz_v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST\n",
    "idx_file = 0\n",
    "with h5py.File(hdf5_feat_file, 'r') as hdf5_fid:\n",
    "    cqt_value_3m = hdf5_fid[audiofile_l[idx_file] + '/cqt_value_3m/'][:]\n",
    "    cqt_time_sec_v = hdf5_fid[audiofile_l[idx_file] + '/cqt_time_sec_v/'][:]\n",
    "    cqt_frequency_hz_v = hdf5_fid[audiofile_l[idx_file] + '/cqt_frequency_hz_v/'][:]\n",
    "\n",
    "print(cqt_value_3m.shape)\n",
    "\n",
    "plt.figure(figsize=(10,6));\n",
    "plt.imshow(flog(cqt_value_3m[param_hcqt.h_l.index(1),:,:]), aspect='auto', origin='lower', interpolation='none', extent=[cqt_time_sec_v[0], cqt_time_sec_v[-1], 0, len(cqt_frequency_hz_v)]);\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining patches\n",
    "\n",
    "The input `X` of our model is actually a temporal patch/chunk of the whole feature matrix `(nb_time, nb_dim)`.\n",
    "For a given feature matrix of length `total_len`, the function `feature.f_get_patches` provides the list of possible patches of length `patch_len` and hop size `patch_hopsize` (we perform frame-analysis over the feature matrix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST\n",
    "feature.f_get_patches(total_len=250, patch_len=64, patch_hopsize=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define mapping\n",
    "\n",
    "A mapping function is a function that will map the annotations to the format imposed in the `y` of the model and the time extent of the `X`.\n",
    "\n",
    "Because Bach10 and MAPS use different annotation systems (frame-based or segment-based), we need to write two different mapping functions: `f_map_annot_frame_based` and `f_map_annot_segment_based`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frame-based f0 annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_map_annot_frame_based(cqt_time_sec_v, cqt_frequency_hz_v, f0multi_entry_l):\n",
    "    \"\"\"\n",
    "    Map f0 annotations to CQT frames and frequencies in the case the ground-truth is given as frame-based f0\n",
    "\n",
    "    Args:\n",
    "        cqt_time_sec_v (nb_frame)\n",
    "        cqt_frequency_hz_v (nb_freq)\n",
    "        f0multi_entry_l contains a multi-beakpoint which fields \n",
    "            'time' (nb_time)\n",
    "            'value' (nb_dim, nb_time)\n",
    "    Returns:\n",
    "        cqt_f0_value_m (nb_freq, nb_frame)\n",
    "    \"\"\"\n",
    "    \n",
    "    nb_freq = len(cqt_frequency_hz_v)\n",
    "    nb_frame = len(cqt_time_sec_v)\n",
    "    cqt_f0_value_m = np.zeros( (nb_freq, nb_frame) )\n",
    "    \n",
    "    gt_time_sec_v = np.asarray(f0multi_entry_l[0]['time'])\n",
    "    gt_f0_midi_m = np.asarray(f0multi_entry_l[0]['value']).T\n",
    "    nb_stem = gt_f0_midi_m.shape[0]\n",
    "    \n",
    "    # --- gt_f0_midi_m (nb_stem, nb_time)\n",
    "    for num_stem in range(nb_stem):\n",
    "        for num_frame in range(nb_frame):\n",
    "            # --- for each time of the CQT we look for the closest ground-truth time\n",
    "            pos_frame = np.argmin(np.abs(gt_time_sec_v - cqt_time_sec_v[num_frame]))\n",
    "            # --- we then look for the closest CQT frequency to the ground-truth f0\n",
    "            gt_f0_hz = midi_to_f0(gt_f0_midi_m[num_stem, pos_frame])\n",
    "            pos_freq = np.argmin(np.abs(gt_f0_hz-cqt_frequency_hz_v))\n",
    "            cqt_f0_value_m[pos_freq, num_frame] = 1\n",
    "    return cqt_f0_value_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segment-based (start, stop, value) f0 annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_map_annot_segment_based(cqt_time_sec_v, cqt_frequency_hz_v, segment_l):\n",
    "    \"\"\"\n",
    "    Map f0 annotations to CQT frames and frequencies in the case the ground-truth is given as segment of notes\n",
    "\n",
    "    Args:\n",
    "        cqt_time_sec_v (nb_frame)\n",
    "        cqt_frequency_hz_v (nb_freq)\n",
    "        segment_l  a list of segments each is a dictionary {'start': , 'stop:', 'value': }\n",
    "            value is given in midi-float unit\n",
    "    Returns:\n",
    "        cqt_f0_value_m (nb_freq, nb_frame)\n",
    "    \"\"\"\n",
    "     \n",
    "    cqt_f0_value_m = np.zeros( (len(cqt_frequency_hz_v), len(cqt_time_sec_v)) )\n",
    "    for note_d in segment_l:\n",
    "        pos_start = np.argmin(np.abs(note_d['time']-cqt_time_sec_v))\n",
    "        pos_stop = np.argmin(np.abs(note_d['time']+note_d['duration']-cqt_time_sec_v))\n",
    "        pos_value = np.argmin(np.abs(midi_to_f0(note_d['value'])-cqt_frequency_hz_v))\n",
    "        cqt_f0_value_m[pos_value,pos_start:pos_stop ] = 1\n",
    "    return cqt_f0_value_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST\n",
    "idx_file = 4\n",
    "with h5py.File(hdf5_audio_file, 'r') as hdf5_fid:\n",
    "    audio_v = hdf5_fid[audiofile_l[idx_file]][:]\n",
    "    sr_hz = hdf5_fid[audiofile_l[idx_file]].attrs['sr_hz']\n",
    "entry = data_d['collection']['entry'][idx_file]\n",
    "print(audiofile_l[idx_file])\n",
    "\n",
    "cqt_d = Namespace()\n",
    "cqt_d.value_3m, cqt_d.time_sec_v, cqt_d.frequency_hz_v = feature.f_get_hcqt(audio_v, sr_hz, param_hcqt)\n",
    "if do_f0_annot == 'frame':      cqt_d.f0_value_m = f_map_annot_frame_based(cqt_d.time_sec_v, cqt_d.frequency_hz_v, entry['f0multi'])\n",
    "elif do_f0_annot == 'segment':  cqt_d.f0_value_m = f_map_annot_segment_based(cqt_d.time_sec_v, cqt_d.frequency_hz_v, entry['pitchmidi'])\n",
    "\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(10,6));\n",
    "D=1/2\n",
    "plt.subplot(111); \n",
    "plt.imshow(flog(cqt_d.value_3m[param_hcqt.h_l.index(1),:,:]), aspect='auto', cmap='gray_r', origin='lower', interpolation='none', extent=[cqt_d.time_sec_v[0], cqt_d.time_sec_v[-1], 0-D, len(cqt_d.frequency_hz_v)-D]);\n",
    "plt.imshow(flog(cqt_d.f0_value_m), aspect='auto', cmap='Reds', origin='lower', interpolation='none', extent=[cqt_d.time_sec_v[0], cqt_d.time_sec_v[-1], 0-D, len(cqt_d.frequency_hz_v)-D], alpha=0.65);\n",
    "plt.xlim(0,10)\n",
    "plt.colorbar();\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "\n",
    "The class `PitchDataset` (a subset of pytorch `Dataset` class) is responsible for providing (with the `__getitem` method) the input `X` and ground-truth `y` of the pitches existing in a patch.\n",
    "\n",
    "- `__getitem__`: for a given `idx_patch`, it provides the `X` (either a patch of CQT or Harmonic-CQT) tensor and the ground-truth binary pitch matrix \n",
    "- '__init__': \n",
    "  - will read the hdf5_feat_file and pyjama_annot_file\n",
    "  - split their content according to training and test\n",
    "  - convert the pitch annotations to the format expected for `y`\n",
    "  - store all the necessary data in memory (of CPU) to fasten later access: `self.data_d[key=audiofile]` for the CQT/H-CQT, `self.patch_l` for the list of all possible patch over all possibe audiofile. The `__getitem__` method simply get a patch, which audiofile it is coming from, and its position within it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class PitchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    description\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hdf5_feat_file, pyjama_annot_file, do_train):\n",
    "\n",
    "        with open(pyjama_annot_file, encoding = \"utf-8\") as json_fid: data_d = json.load(json_fid)\n",
    "        entry_l = data_d['collection']['entry']\n",
    "        \n",
    "        # --- only keep entries with annotation field 'f0multi'\n",
    "        if do_f0_annot == 'frame':\n",
    "            entry_l = [entry for entry in entry_l if len(entry['f0multi'])]\n",
    "\n",
    "        self.do_train = do_train\n",
    "        if self.do_train:   entry_l = [entry_l[idx] for idx in range(len(entry_l)) if (idx % 5) != 0]\n",
    "        else:               entry_l = [entry_l[idx] for idx in range(len(entry_l)) if (idx % 5) == 0]\n",
    "\n",
    "        self.data_d = {}\n",
    "        self.patch_l = []\n",
    "        \n",
    "        audio_d = Namespace()\n",
    "        cqt_d = Namespace()\n",
    "        gt_d = Namespace()\n",
    "\n",
    "        with h5py.File(hdf5_feat_file, 'r') as feat_fid:\n",
    "            for entry in tqdm(entry_l):\n",
    "                audio_file = entry['filepath'][0]['value']\n",
    "                \n",
    "                # --- get features\n",
    "                cqt_d.value_3m = feat_fid['/' +  audio_file + '/cqt_value_3m/'][:]\n",
    "                cqt_d.time_sec_v = feat_fid['/' +  audio_file + '/cqt_time_sec_v/'][:]\n",
    "                cqt_d.frequency_hz_v = feat_fid['/' +  audio_file + '/cqt_frequency_hz_v/'][:]\n",
    "\n",
    "                # --- map annotations\n",
    "                if do_f0_annot == 'frame':      cqt_d.f0_value_m = f_map_annot_frame_based(cqt_d.time_sec_v, cqt_d.frequency_hz_v, entry['f0multi'])\n",
    "                elif do_f0_annot == 'segment':  cqt_d.f0_value_m = f_map_annot_segment_based(cqt_d.time_sec_v, cqt_d.frequency_hz_v, entry['pitchmidi'])\n",
    "\n",
    "\n",
    "                # --- store for later use\n",
    "                self.data_d[audio_file] = {'X': torch.tensor(cqt_d.value_3m).float(), 'y': torch.tensor(cqt_d.f0_value_m)}\n",
    "                \n",
    "                # --- create list of patches and associate information\n",
    "                localpatch_l = feature.f_get_patches(cqt_d.value_3m.shape[2], param_patch.L_frame, param_patch.STEP_frame)\n",
    "                for localpatch in localpatch_l:\n",
    "                    self.patch_l.append({'audiofile': audio_file,\n",
    "                                        'start_frame': localpatch['start_frame'],\n",
    "                                        'end_frame': localpatch['end_frame'],\n",
    "                                        })\n",
    "            self.cqt_d_frequency_hz_v = cqt_d.frequency_hz_v\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.patch_l)\n",
    "\n",
    "    def __getitem__(self, idx_patch):\n",
    "        audiofile = self.patch_l[idx_patch]['audiofile']\n",
    "        s = self.patch_l[idx_patch]['start_frame']\n",
    "        e = self.patch_l[idx_patch]['end_frame']\n",
    "        \n",
    "        X = self.data_d[ audiofile ]['X'][:,:, s:e]\n",
    "        y = self.data_d[ audiofile ]['y'][:, s:e]\n",
    "        return {'X':X , 'y':y}\n",
    "    \n",
    "train_dataset = PitchDataset(hdf5_feat_file, pyjama_annot_file, do_train=True)\n",
    "valid_dataset = PitchDataset(hdf5_feat_file, pyjama_annot_file, do_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST\n",
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))\n",
    "\n",
    "idx_path = 51\n",
    "plt.subplot(121); plt.imshow(flog(train_dataset[idx_path]['X'][param_hcqt.h_l.index(1),:,:]).numpy(), aspect='auto', origin='lower', interpolation='none', );\n",
    "plt.subplot(122); plt.imshow(train_dataset[idx_path]['y'][:,:].numpy(), aspect='auto', origin='lower', interpolation='none', );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader\n",
    "\n",
    "We create the dataloader for the training and validation data from the corresponding dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=param_model.batch_size, shuffle=True, num_workers=9)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=param_model.batch_size, shuffle=False, num_workers=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch['X'].size())\n",
    "print(batch['y'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The pytorch model is generated automatically based on the content of the `config_bittner.yaml` (ConvNet) or `config_doras.yaml` (U-Net) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet-model (based on Bittner) or U-Net-model (bsed on Doras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'config_bittner.yaml'\n",
    "#config_file = 'config_doras.yaml'\n",
    "\n",
    "with open(config_file, 'r') as fid: \n",
    "    cfg_dic = yaml.safe_load(fid)\n",
    "config = munchify(cfg_dic)\n",
    "m, C, H, T = 32, len(param_hcqt.h_l), 360, 64\n",
    "\n",
    "model = model_factory.NetModel(config, [m, C, H, T])\n",
    "model = model.to(param_model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the model\n",
    "\n",
    "We can check the model either using\n",
    "- print(model)\n",
    "- the `verbose` mode of the `forward` of the model\n",
    "- the `torchsummary.summary` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST\n",
    "# #print(model)\n",
    "\n",
    "#X = torch.randn(m, C, H, T).to(param_model.device)\n",
    "#print(model(X, True).size())\n",
    "\n",
    "torchsummary.summary(model, (C, H, T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "We can also test the model and the loss, i.e. checking that the format of the `X` corresponds to what the model is expected; and the format of the `y` and `hat_y` corresponds to what the loss is expected.\n",
    "If this is OK we can start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "X = batch['X'].to(param_model.device)\n",
    "y = batch['y'].to(param_model.device)\n",
    "hat_y = model(X)\n",
    "print(f'{X.size()} {y.size()} {hat_y.squeeze(1).size()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BCEWithLogitsLoss(reduction='none')(hat_y.squeeze(1), y).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using torchlighning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W&B configuration\n",
    "\n",
    "We configure `https://wandb.ai/` to monitor the training of our model.\n",
    "As `tensorboard`, `wandb` is a server that keep track of the performance in real-time of a raining. \n",
    "Unlike `tensorboard`, `wandb` server runs online and can therefore be accessed from anywhere (you can monitor your training in the bus, train, metro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config_d = {}\n",
    "current_datetime = datetime.datetime.now()\n",
    "formatted_datetime = current_datetime.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "expe_name = formatted_datetime\n",
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project = param_wandb.project_name, name = expe_name, save_dir = param_wandb.save_dir )\n",
    "wandb_logger.experiment.config.update(train_config_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TorchLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PitchLigthing(pl.LightningModule):\n",
    "    def __init__(self, in_model):\n",
    "        super().__init__()\n",
    "        self.model = in_model\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        hat_y = self.model(batch['X'])\n",
    "        loss = self.loss(hat_y.squeeze(1), batch['y'])\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        hat_y = self.model(batch['X'])\n",
    "        loss = self.loss(hat_y.squeeze(1), batch['y'])\n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "\n",
    "        if batch_idx==0: \n",
    "            data_image = np.hstack( (batch['y'][0,:,:].cpu().numpy(),  nn.Sigmoid()(hat_y[0,0,:,:].detach()).cpu().numpy() ) )\n",
    "            #wandb.log({'val3/f0': wandb.Image( data_image )})\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), 0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We train the model and apply a early-stopping based on the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lighting = PitchLigthing(model)\n",
    "early_stop_callback = EarlyStopping(monitor=\"valid_loss\", patience=10, verbose=True, mode=\"min\")\n",
    "checkpoint_callback = ModelCheckpoint(monitor='valid_loss', dirpath=param_lightning.dirpath, filename=param_lightning.filename, save_top_k=1, mode='min')\n",
    "#trainer = pl.Trainer(accelerator=\"gpu\",  logger = wandb_logger, max_epochs = param_lightning.max_epochs, callbacks = [early_stop_callback, checkpoint_callback])\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",  max_epochs = param_lightning.max_epochs, callbacks = [early_stop_callback, checkpoint_callback])\n",
    "trainer.fit(model=my_lighting, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We first load the best model obtained during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best model saved at: {best_model_path}\")\n",
    "my_lighting = PitchLigthing.load_from_checkpoint(best_model_path, in_model=model)\n",
    "\n",
    "print( type(model) )\n",
    "print( type(my_lighting) )\n",
    "print( type(my_lighting.model) )\n",
    "\n",
    "model = my_lighting.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration\n",
    "\n",
    "We illustrate the results obtained by the trained model on a given audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(param_model.device)\n",
    "data = next(iter(valid_dataloader))\n",
    "\n",
    "hat_y = nn.Sigmoid()( model(data['X'].to(param_model.device)) )\n",
    "\n",
    "print(f\"{data['X'].size()} {data['y'].size()} {hat_y.size()}\")\n",
    "\n",
    "idx_patch = 0\n",
    "\n",
    "plt.figure(figsize=(10,6));\n",
    "plt.subplot(211); plt.imshow(data['y'][idx_patch,:,:].cpu().numpy(), cmap='gray_r', aspect='auto'); plt.colorbar();\n",
    "plt.subplot(212); plt.imshow(hat_y[idx_patch,0,:,:].detach().cpu().numpy(), cmap='gray_r', aspect='auto'); plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Pitch: performance measures using mir_eval\n",
    "\n",
    "We send all the data of the validation dataset to the model, get the estimated pitch `hat_y` and change its format t be able to use `mir_eval.multipitch.evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "threshold = 0.5\n",
    "\n",
    "model.to(param_model.device)\n",
    "\n",
    "store_l = []\n",
    "for data in valid_dataloader:\n",
    "    hat_y = nn.Sigmoid()( model(data['X'].to(param_model.device)) )\n",
    "    \n",
    "    nb_patch = hat_y.size(0)\n",
    "    for idx_patch in range(nb_patch):\n",
    "        gt_m = data['y'][idx_patch,:,:].cpu().numpy()\n",
    "        est_m = hat_y[idx_patch,0,:,:].detach().cpu().numpy()\n",
    "        ref_time_l, est_time_l, ref_freqs_l, est_freqs_l = [], [], [], []\n",
    "\n",
    "        for t in range(gt_m.shape[1]):\n",
    "            ref_time_l.append( float(t) )\n",
    "            ref_freqs_l.append( valid_dataset.cqt_d_frequency_hz_v[ gt_m[:,t]>0 ] )\n",
    "            est_time_l.append( float(t) )\n",
    "            est_freqs_l.append( valid_dataset.cqt_d_frequency_hz_v[ est_m[:,t]>threshold ] )\n",
    "\n",
    "        ref_time_l = np.array(ref_time_l)\n",
    "        est_time_l = np.array(est_time_l)\n",
    "\n",
    "        dict_d = mir_eval.multipitch.evaluate(ref_time_l, ref_freqs_l, est_time_l, est_freqs_l)\n",
    "        if len(store_l)==0: key_l = [key for key in dict_d.keys()]\n",
    "        dict_d['gt_m'] = gt_m\n",
    "        dict_d['est_m'] = est_m\n",
    "        store_l.append(dict_d)\n",
    "\n",
    "for key in key_l: print(f\"{key:30}: {np.mean([store[key] for store in store_l])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'Accuracy'\n",
    "pos = np.argmin([store[key] for store in store_l])\n",
    "\n",
    "plt.figure(figsize=(10,6));\n",
    "plt.subplot(111); \n",
    "plt.imshow(store_l[pos]['gt_m'], cmap='gray_r', aspect='auto'); \n",
    "plt.imshow(store_l[pos]['est_m']>0.5, cmap='Reds', aspect='auto', alpha=0.3); \n",
    "plt.colorbar()\n",
    "plt.title(store_l[pos][key]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_gpeeters_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
