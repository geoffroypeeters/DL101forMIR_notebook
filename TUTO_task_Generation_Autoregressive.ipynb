{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Unified Jupyter Notebook for Discrete Audio Generation\n",
    "\n",
    "# ## Install Necessary Libraries\n",
    "# First, we need to install the required libraries for the project.\n",
    "!pip install torchaudio x-transformers encodec"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed24ebe72fa28ab7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "# Import necessary modules for training and evaluation.\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from x_transformers import TransformerWrapper, Decoder\n",
    "from encodec import EncodecModel\n",
    "from torch import nn\n",
    "import soundfile as sf\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from encodec.utils import convert_audio\n",
    "import IPython.display as ipd"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ad1ed5eafd5887e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ## Set Parameters\n",
    "# Define some global parameters for the project.\n",
    "LEVELS = 2\n",
    "TIMESTEPS = 125\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T14:01:11.279877Z",
     "start_time": "2024-10-14T14:01:08.339641Z"
    }
   },
   "id": "d835554afd0c625b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ## Dataset Class Definition\n",
    "# Define a dataset class to handle discrete audio representations.\n",
    "class DiscreteAudioRepDataset(Dataset):\n",
    "    def __init__(self, root_dir, model, lazy_encode=True, extensions=[\".wav\", \".mp3\", \".flac\"]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the audio files.\n",
    "            model: The EnCodec model for encoding audio.\n",
    "            lazy_encode (bool): If True, encodes audio on-demand (when __getitem__ is called).\n",
    "                               If False, encodes all audio at initialization.\n",
    "            extensions (list): List of audio file extensions to include.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.extensions = extensions\n",
    "        self.model = model\n",
    "        self.lazy_encode = lazy_encode\n",
    "        self.audio_files = []\n",
    "\n",
    "        # Walk through all subfolders to gather audio files\n",
    "        for root, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if any(file.endswith(ext) for ext in self.extensions):\n",
    "                    self.audio_files.append(os.path.join(root, file))\n",
    "\n",
    "        # If not lazy encoding, encode all audio files during initialization\n",
    "        if not self.lazy_encode:\n",
    "            self.encoded_data = [self._encode_audio(file) for file in tqdm(self.audio_files, desc=\"Encoding Audio\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.lazy_encode:\n",
    "            filename = self.audio_files[idx]\n",
    "            return self._encode_audio(filename)[..., :TIMESTEPS*LEVELS]\n",
    "        else:\n",
    "            encoded_audio = self.encoded_data[idx]\n",
    "            return encoded_audio[:TIMESTEPS * LEVELS]\n",
    "\n",
    "    def _encode_audio(self, filename):\n",
    "        waveform, sample_rate = sf.read(filename)\n",
    "        waveform = torch.tensor(waveform, dtype=torch.float32)[None, None, :]  # Add batch dimension\n",
    "        waveform = convert_audio(waveform, sample_rate, self.model.sample_rate, self.model.channels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            discrete_reps = self.model.encode(waveform.to(device))\n",
    "\n",
    "        discrete_reps = discrete_reps[0][0].contiguous().permute(0, 2, 1).reshape(-1)\n",
    "        return discrete_reps.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T14:01:22.812167Z",
     "start_time": "2024-10-14T14:01:22.798851Z"
    }
   },
   "id": "dfa347dadda60b06",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ## Load EnCodec Model\n",
    "# Load the EnCodec model to transform the audio representation.\n",
    "codec = EncodecModel.encodec_model_24khz().to(device)\n",
    "codec.set_target_bandwidth(1.5)\n",
    "\n",
    "# ## Load Dataset\n",
    "# Load the NSYNTH dataset and prepare DataLoader for training and validation.\n",
    "audio_folder_train = \"./NSYNTH/nsynth-train\"\n",
    "audio_folder_val = \"./NSYNTH/nsynth-valid\"\n",
    "\n",
    "dataset = DiscreteAudioRepDataset(root_dir=audio_folder_train, model=codec, lazy_encode=False)\n",
    "dataloader = DataLoader(dataset, batch_size=350, shuffle=True)\n",
    "\n",
    "dataset_val = DiscreteAudioRepDataset(root_dir=audio_folder_val, model=codec, lazy_encode=False)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=350, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8697a533d75bba2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ## Define Transformer Model\n",
    "# Define the Transformer model using TransformerWrapper and Decoder.\n",
    "model = TransformerWrapper(\n",
    "    emb_dropout=0.1,\n",
    "    num_tokens=1024,\n",
    "    max_seq_len=LEVELS*TIMESTEPS,\n",
    "    attn_layers=Decoder(\n",
    "        dim=256,\n",
    "        depth=6,\n",
    "        heads=4,\n",
    "        rotary_pos_emb=True,\n",
    "        attn_dropout=0.1,\n",
    "        ff_dropout=0.1\n",
    "    )\n",
    ").to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a56a1ecdea2489cf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Optionally load pretrained weights\n",
    "model.load_state_dict(torch.load('./model_weights/model_gen_autoreg_transformer.pth', map_location=device))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "482149b360b0fc86",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ## Training Loop\n",
    "# Define a training loop for training the Transformer model.\n",
    "epochs = 1000\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    train_loss, val_loss = 0, 0\n",
    "    model.train()\n",
    "    total_correct, total_predictions = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        if batch.dtype != torch.long:\n",
    "            batch = batch.long()\n",
    "        start_tokens = torch.zeros((batch.shape[0], 1), dtype=torch.long, device=batch.device)\n",
    "        batch = torch.cat([start_tokens, batch], dim=1)\n",
    "        discrete_reps = batch.to(device)\n",
    "\n",
    "        logits = model(discrete_reps)\n",
    "        logits = logits.permute(0, 2, 1)\n",
    "        loss = criterion(logits[..., :-1], discrete_reps[..., 1:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "        preds = logits[..., :-1].argmax(dim=1)\n",
    "        targets = discrete_reps[..., 1:]\n",
    "        correct = (preds == targets).sum().item()\n",
    "        total_correct += correct\n",
    "        total_predictions += targets.numel()\n",
    "\n",
    "    avg_train_loss = train_loss / count\n",
    "    precision = total_correct / total_predictions\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Precision: {precision:.4f}')\n",
    "    torch.save(model.state_dict(), 'model.pth')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "189139114dcf76f9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ## Evaluation and Audio Generation\n",
    "# Evaluate the model and generate audio from the trained Transformer.\n",
    "model.eval()\n",
    "num_samples = 5\n",
    "seq_len = LEVELS * TIMESTEPS\n",
    "temperature = 1.0\n",
    "os.makedirs('generated_audio', exist_ok=True)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    print(f\"Generating sample {i+1}/{num_samples}\")\n",
    "    start_token = 0\n",
    "    generated = [start_token]\n",
    "    for _ in tqdm(range(seq_len), desc=\"Generating Tokens\", leave=False):\n",
    "        input_seq = torch.tensor([generated], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_seq)[:, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated.append(next_token)\n",
    "    generated_sequence = torch.tensor(generated[1:], dtype=torch.long).to(device)\n",
    "    codes = generated_sequence.view(1, -1, LEVELS).transpose(1, 2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        decoded_audio = codec.decode([(codes, None)])\n",
    "    decoded_audio = decoded_audio.squeeze().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    output_filename = f'generated_audio/sample_{i+1}.wav'\n",
    "    sf.write(output_filename, decoded_audio, samplerate=codec.sample_rate)\n",
    "    print(f\"Saved {output_filename}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad553e4d9d25a18f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ## Play Generated Audio \n",
    "# Use IPython audio player to play generated audio samples.\n",
    "for i in range(1, num_samples + 1):\n",
    "    output_filename = f'generated_audio/sample_{i}.wav'\n",
    "    print(f\"Playing {output_filename}\")\n",
    "    ipd.display(ipd.Audio(output_filename))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e961682e0d234a28"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
