{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Auto-Tagging using different front-end/ different architecture\n",
    "\n",
    "- date: 2024-10-10\n",
    "- author: geoffroy.peeters@telecom-paris.fr\n",
    "\n",
    "This notebook was created for the tutorial held at ISMIR-2024 \"Deep-Learning 101 for Audio-based Music Information Retrieval\".\n",
    "It illustrates the use of various deep-learning bricks to solve the task of \"Auto-Tagging\".\n",
    "\n",
    "Part of the code is based on \n",
    "- SincNet model: https://github.com/mravanelli/SincNet/blob/master/dnn_models.py \n",
    "\n",
    "Datasets are available at\n",
    "- GTZAN-Genre \n",
    "- MTT (Magna-Tag-a-Tune) https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset\n",
    "\n",
    "The two datasets illustrate different type of problems.\n",
    "- GTZAN-Genre is a multi-class problem (classification into 10 mutually exclusive classes)\n",
    "- MTT (Magna-Tag-a-Tune) is a multi-label (classification into 10 non-mutually exclusive tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "In case the notebook is run on GoogleColab or others, we first need to\n",
    "- get the packages: `git clone``\n",
    "- get the datasets\n",
    "\n",
    "We also test the models on two different datasets:\n",
    "- GTZAN-Genre\n",
    "- MTT (Magna-Tag-A-Tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_deploy = False\n",
    "\n",
    "if do_deploy:\n",
    "    !git clone https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook.git\n",
    "    %cd deeplearning-101-audiomir_notebook\n",
    "    !ls\n",
    "\n",
    "    import urllib.request\n",
    "    import shutil\n",
    "    ROOT = 'https://perso.telecom-paristech.fr/gpeeters/tuto_DL101forMIR/'\n",
    "    \n",
    "    #hdf5_audio_file, pyjama_annot_file = 'gtzan-genre_audio.hdf5.zip', 'gtzan-genre.pyjama'\n",
    "    hdf5_audio_file, pyjama_annot_file = 'mtt_audio.hdf5', 'mtt.pyjama'\n",
    "    \n",
    "    urllib.request.urlretrieve(ROOT + hdf5_audio_file, hdf5_audio_file)\n",
    "    if hdf5_audio_file.endswith('.zip'): shutil.unpack_archive(hdf5_audio_file, './')\n",
    "    urllib.request.urlretrieve(ROOT + pyjama_annot_file, pyjama_annot_file)\n",
    "\n",
    "    ROOT = './'\n",
    "\n",
    "else:\n",
    "\n",
    "    ROOT = '/tsi/data_doctorants/gpeeters/_data/'\n",
    "\n",
    "base, problem, annot_key, n_out = 'gtzan-genre', 'multiclass', 'genre', 10\n",
    "#base, problem, annot_key, n_out = 'mtt', 'multilabel', 'tag', 50\n",
    "\n",
    "config_file = 'config_autotagging.yaml'\n",
    "do_feature = 'lms'#, 'waveform', 'lms'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torchsummary\n",
    "\n",
    "! pip install lightning --quiet\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "! pip install wandb --quiet\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "import h5py\n",
    "import pprint as pp\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import feature\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "import IPython.display\n",
    "\n",
    "# -----------------------------\n",
    "import model_factory \n",
    "import importlib\n",
    "importlib.reload(model_factory) \n",
    "\n",
    "from argparse import Namespace\n",
    "! pip install munch --quiet\n",
    "from munch import munchify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set fixed seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      2\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mmanual_seed(seed)         \u001b[38;5;66;03m# For CPU\u001b[39;00m\n\u001b[1;32m      4\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed)               \u001b[38;5;66;03m# For Python's built-in random module\u001b[39;00m\n\u001b[1;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)            \u001b[38;5;66;03m# For NumPy\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "seed = 42\n",
    "torch.manual_seed(seed)         # For CPU\n",
    "random.seed(seed)               # For Python's built-in random module\n",
    "np.random.seed(seed)            # For NumPy\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)          # For current GPU\n",
    "    torch.cuda.manual_seed_all(seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_audio_file = f'{ROOT}/{base}_audio.hdf5'\n",
    "pyjama_annot_file = f'{ROOT}/{base}.pyjama'\n",
    "\n",
    "\n",
    "param_lms = Namespace()\n",
    "param_lms.nb_band = 128\n",
    "param_lms.L_n = 2048\n",
    "param_lms.STEP_n = 1024\n",
    "\n",
    "param_patch = Namespace()\n",
    "param_model = Namespace()\n",
    "if do_feature == 'waveform':\n",
    "    param_patch.L_frame = 3200\n",
    "    param_model.n_in = 1\n",
    "elif do_feature == 'lms':\n",
    "    param_patch.L_frame = 64\n",
    "    param_model.n_in = param_lms.nb_band\n",
    "param_patch.STEP_frame = int(param_patch.L_frame/2)\n",
    "\n",
    "param_model.T_in = param_patch.L_frame\n",
    "param_model.n_out = n_out\n",
    "param_model.batch_size = 128\n",
    "param_model.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "param_lightning = Namespace()\n",
    "param_lightning.max_epochs = 500\n",
    "param_lightning.dirpath='_autotagging_lighning/'\n",
    "param_lightning.filename='best_model_autotagging'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loading pyjama/hdf5\n",
    "\n",
    "All the audio data of a dataset are stored in a single [.hdf5](https://docs.h5py.org/) file.\n",
    "Each `key` corresponds to an entry, a specific audiofile.\n",
    "Its array contains the audio wavform and the attribute `sr_hz` provides its sampling rate.\n",
    "\n",
    "All the annotations of a dataset are stored in a single *.pyjama file.\n",
    "As [JAMS](https://github.com/marl/jams) files, .pyjama files are JSON files.\n",
    "However, a single .pyjama file can contain the annotations of ALL entries of a dataset.\n",
    "Its specifications are described here [DOC](https://github.com/geoffroypeeters/pyjama).\n",
    "The values of the `filepath` field of the .pyjama file correspond to the `key` values of the .hdf5 file.\n",
    "\n",
    "We want to solve `multi-class` and `multi-label` problem with the same code. However, in the former we have only one label per item while we can have several for the later. TO deal with this we define the functions `f_get_labelname_dict` and `f_get_groundtruth_item` that provides the list of unique labels and the labels of a given item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_labelname_dict(data_d, annot_key):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        provides the dictionary of labelname used in a given corpus\n",
    "    \"\"\"\n",
    "    labelname_dict_l = []\n",
    "    for entry in data_d['collection']['entry']:\n",
    "        for annot in entry[annot_key]:\n",
    "            labelname_dict_l.append(annot['value'])\n",
    "    labelname_dict_l = list(set(labelname_dict_l))\n",
    "    return sorted(labelname_dict_l)\n",
    "\n",
    "def f_get_groundtruth_item(entry, annot_key, labelname_dict_l, problem):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        map a label (or a list of labels) to a ground-truth\n",
    "    \"\"\"\n",
    "    if problem=='multiclass':\n",
    "        annot = entry[annot_key][0]\n",
    "        idx_label = labelname_dict_l.index(annot['value'])\n",
    "    elif problem=='multilabel':\n",
    "        idx_label = np.zeros(len(labelname_dict_l))\n",
    "        for annot in entry[annot_key]:\n",
    "            pos = labelname_dict_l.index(annot['value'])\n",
    "            idx_label[ pos ] = 1\n",
    "    return idx_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pyjama_annot_file, encoding = \"utf-8\") as json_fid:\n",
    "    data_d = json.load(json_fid)\n",
    "entry_l = data_d['collection']['entry']\n",
    "\n",
    "pp.pprint(entry_l[0:2])\n",
    "\n",
    "\n",
    "audiofile_l = [entry['filepath'][0]['value'] for entry in entry_l]\n",
    "print(f'number of audio: {len(audiofile_l)}')\n",
    "pp.pprint(audiofile_l[:5])\n",
    "\n",
    "\n",
    "labelname_dict_l = f_get_labelname_dict(data_d, annot_key)\n",
    "print(f'number of tags: {len(labelname_dict_l)}')\n",
    "pp.pprint(labelname_dict_l[:5])\n",
    "\n",
    "\n",
    "with h5py.File(hdf5_audio_file, 'r') as hdf5_fid:\n",
    "    #audiofile_l = [key for key in hdf5_fid['/'].keys()]\n",
    "    pp.pprint(f\"audio shape: {hdf5_fid[audiofile_l[0]][:].shape}\")\n",
    "    pp.pprint(f\"audio sample-rate: {hdf5_fid[audiofile_l[0]].attrs['sr_hz']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define features\n",
    "\n",
    "We define the feature that will be used as input to the model, the `X`, and test it on an audio file.\n",
    "We will use here Log-Mel-Spectrogram provided by the function `feature.f_get_lms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST\n",
    "idx_file = 100\n",
    "with h5py.File(hdf5_audio_file, 'r') as hdf5_fid:\n",
    "    print(audiofile_l[idx_file])\n",
    "    audio_v = hdf5_fid[audiofile_l[idx_file]][:]\n",
    "    sr_hz = hdf5_fid[audiofile_l[idx_file]].attrs['sr_hz']\n",
    "\n",
    "data_m, time_sec_v = feature.f_get_lms(audio_v, sr_hz, param_lms)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "print(data_m.shape)\n",
    "plt.imshow(data_m, origin='lower', aspect='auto'); plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining patches\n",
    "\n",
    "The input `X` of our model is actually a temporal patch/chunk of the whole feature matrix `(nb_time, nb_dim)`.\n",
    "For a given feature matrix of length `total_len`, the function `feature.f_get_patches` provides the list of possible patches of length `patch_len` and hop size `patch_hopsize` (we perform frame-analysis over the feature matrix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST\n",
    "feature.f_get_patches(total_len=250, patch_len=64, patch_hopsize=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "\n",
    "The class `PitchDataset` (a subset of pytorch `Dataset` class) is responsible for providing (with the `__getitem` method) the input `X` and ground-truth `y` of the pitches existing in a patch.\n",
    "\n",
    "- `__getitem__`: for a given `idx_patch`, it provides the `X` (either a patch of CQT or Harmonic-CQT) tensor and the ground-truth binary pitch matrix \n",
    "- '__init__': \n",
    "  - will read the hdf5_feat_file and pyjama_annot_file\n",
    "  - split their content according to training and test\n",
    "  - convert the tag annotations to the format expected for `y`\n",
    "  - store all the necessary data in memory (of CPU) to fasten later access: `self.data_d[key=audiofile]` for the CQT/H-CQT, `self.patch_l` for the list of all possible patch over all possibe audiofile. The `__getitem__` method simply get a patch, which audiofile it is coming from, and its position within it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class TagDataset(Dataset):\n",
    "    \"\"\"\n",
    "    description\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hdf5_audio_file, pyjama_annot_file, do_train):\n",
    "\n",
    "        with open(pyjama_annot_file, encoding = \"utf-8\") as json_fid: data_d = json.load(json_fid)\n",
    "        entry_l = data_d['collection']['entry']\n",
    "        \n",
    "        #entry_l = entry_l[:1000] ############### TO REMOVE ####################\n",
    "        \n",
    "        self.labelname_dict_l = f_get_labelname_dict(data_d, annot_key)\n",
    "\n",
    "        self.do_train = do_train\n",
    "        if self.do_train:   entry_l = [entry_l[idx] for idx in range(len(entry_l)) if (idx % 5) != 0]\n",
    "        else:               entry_l = [entry_l[idx] for idx in range(len(entry_l)) if (idx % 5) == 0]\n",
    "\n",
    "        self.audio_file_l =  [entry['filepath'][0]['value'] for entry in entry_l]\n",
    "\n",
    "        self.data_d = {}\n",
    "        self.patch_l = []\n",
    "        \n",
    "        with h5py.File(hdf5_audio_file, 'r') as audio_fid:\n",
    "            for entry in tqdm(entry_l):\n",
    "                audio_file= entry['filepath'][0]['value']\n",
    "\n",
    "                # --- get features\n",
    "                if do_feature == 'waveform':    feat_value_m = audio_fid[audio_file][:].reshape(1,-1)\n",
    "                elif do_feature == 'lms':       feat_value_m, _ = feature.f_get_lms(audio_fid[audio_file][:], audio_fid[audio_file].attrs['sr_hz'], param_lms)\n",
    "                \n",
    "                # --- map annotations\n",
    "                idx_label = f_get_groundtruth_item(entry, annot_key, self.labelname_dict_l, problem)\n",
    "                \n",
    "                # --- store for later use\n",
    "                self.data_d[audio_file] = {'X': torch.tensor(feat_value_m).float(), 'y': torch.tensor(idx_label)}\n",
    "                \n",
    "                # --- create list of patches and associate information\n",
    "                localpatch_l = feature.f_get_patches(feat_value_m.shape[-1], param_patch.L_frame, param_patch.STEP_frame)\n",
    "                for localpatch in localpatch_l:\n",
    "                    self.patch_l.append({'audiofile': audio_file,\n",
    "                                        'start_frame': localpatch['start_frame'],\n",
    "                                        'end_frame': localpatch['end_frame'],\n",
    "                                        })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patch_l)\n",
    "\n",
    "    def __getitem__(self, idx_patch):\n",
    "        audiofile = self.patch_l[idx_patch]['audiofile']        \n",
    "        s = self.patch_l[idx_patch]['start_frame']\n",
    "        e = self.patch_l[idx_patch]['end_frame']\n",
    "\n",
    "        X = self.data_d[ audiofile ]['X'][:,s:e]\n",
    "        if do_feature == 'lms': X = X.unsqueeze(0) # --- add channel dimension\n",
    "        y = self.data_d[ audiofile ]['y'] # --- We suppose the same annotation for the whole file\n",
    "        return {'X':X , 'y':y}\n",
    "    \n",
    "train_dataset = TagDataset(hdf5_audio_file, pyjama_annot_file, do_train=True)\n",
    "valid_dataset = TagDataset(hdf5_audio_file, pyjama_annot_file, do_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0]['X'].shape)\n",
    "print(train_dataset[0]['y'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader\n",
    "\n",
    "We create the dataloader for the training and validation data from the corresponding dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=param_model.batch_size, shuffle=True, num_workers=8, drop_last = True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=param_model.batch_size, shuffle=False, num_workers=8, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch['X'].size())\n",
    "print(batch['y'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The pytorch model is generated automatically based on the content of the `config_autotagging.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file, 'r') as fid: \n",
    "    cfg_dic = yaml.safe_load(fid)\n",
    "config = munchify(cfg_dic)\n",
    "config.model.block_l[-1].sequential_l[-1].layer_l[-1][1].out_features = param_model.n_out\n",
    "pp.pprint(config)\n",
    "\n",
    "if do_feature == 'waveform':\n",
    "    m, C, T = 64, 1, 3200\n",
    "    model = model_factory.NetModel(config, [m, C, T])\n",
    "if do_feature == 'lms':\n",
    "    m, C, H, W = 64, 1, 128, 64\n",
    "    model = model_factory.NetModel(config, [m, C, H, W])\n",
    "\n",
    "model = model.to(param_model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the model\n",
    "\n",
    "We can check the model either using\n",
    "- print(model)\n",
    "- the `verbose` mode of the `forward` of the model\n",
    "- the `torchsummary.summary` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST\n",
    "if do_feature == 'waveform':\n",
    "    m, C, T = 64, 1, 3200\n",
    "\n",
    "    #X = torch.randn(m, C, T).to(param_model.device)\n",
    "    #print(model(X, True).size())\n",
    "    \n",
    "    torchsummary.summary(model, input_size=(C, T))\n",
    "\n",
    "if do_feature == 'lms':\n",
    "    m, C, H, W = 64, 1, 128, 64\n",
    "\n",
    "    #X = torch.randn(m, C, H, W).to(param_model.device)\n",
    "    #print(model(X, True).size())\n",
    "\n",
    "    torchsummary.summary(model, input_size=(C, H, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "We can also test the model and the loss, i.e. checking that the format of the `X` corresponds to what the model is expected; and the format of the `y` and `hat_y` corresponds to what the loss is expected.\n",
    "If this is OK we can start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "X = batch['X'].to(param_model.device)\n",
    "y = batch['y'].to(param_model.device)\n",
    "hat_y = model(X)\n",
    "print(f'{X.size()} {y.size()} {hat_y.squeeze(1).size()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if problem=='multiclass': \n",
    "    nn.CrossEntropyLoss()(hat_y, y)\n",
    "elif problem=='multilabel': \n",
    "    nn.BCEWithLogitsLoss(reduction='none')(hat_y.squeeze(1), y).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using TorchLightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TorchLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(hat_y, y):\n",
    "    \"\"\" Manually compute accuracy \"\"\"\n",
    "    preds = torch.argmax(hat_y, dim=1)  # Get the predicted class (index of max logit)\n",
    "    correct = (preds == y).float()  # Compare with ground truth and cast to float\n",
    "    accuracy = correct.sum() / len(correct)  # Compute mean accuracy over the batch\n",
    "    return accuracy\n",
    "\n",
    "class AutoTaggingLigthing(pl.LightningModule):\n",
    "    def __init__(self, in_model):\n",
    "        super().__init__()\n",
    "        self.model = in_model\n",
    "        if problem=='multiclass': self.loss = nn.CrossEntropyLoss()\n",
    "        elif problem=='multilabel': self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        hat_y = self.model(batch['X'])\n",
    "        loss = self.loss(hat_y, batch['y'])\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        if problem=='multiclass': \n",
    "            accuracy = get_accuracy(hat_y, batch['y'])\n",
    "            self.log('train_acc', accuracy, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        hat_y = self.model(batch['X'])\n",
    "        loss = self.loss(hat_y, batch['y'])\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        if problem=='multiclass': \n",
    "            accuracy = get_accuracy(hat_y, batch['y'])\n",
    "            self.log('val_acc', accuracy, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), 0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We train the model and apply a early-stopping based on the validation loss. We also monitor the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lighting = AutoTaggingLigthing( model )\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=True, mode=\"min\")\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath=param_lightning.dirpath, filename=param_lightning.filename, save_top_k=1, mode='min')\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",  max_epochs = param_lightning.max_epochs, callbacks = [early_stop_callback, checkpoint_callback])\n",
    "trainer.fit(model=my_lighting, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We first load the best model obtained during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best model saved at: {best_model_path}\")\n",
    "my_lighting = AutoTaggingLigthing.load_from_checkpoint(best_model_path, in_model=model)\n",
    "\n",
    "print( type(model) )\n",
    "print( type(my_lighting) )\n",
    "print( type(my_lighting.model) )\n",
    "\n",
    "model = my_lighting.model\n",
    "model.to(param_model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class/multi-label: performance measures using scikitlearn\n",
    "\n",
    "We send all the data of the validation dataset to the model, get the estimated pitch `hat_y`.\n",
    "\n",
    "- For **multiclass**, we then choose the most-likely class for each item `np.argmax` and compute the `recall`, `precision`, `f-measure`, `accuracy`.\n",
    "- For **multilabel**, we preserve the probability outputs, and evaluate the results for various possible threshold using the `roc_auc_score` and `average_precision_score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(problem)\n",
    "if problem=='multiclass':\n",
    "    y_idx_l = []\n",
    "    hat_y_idx_l = []\n",
    "    for batch in valid_dataloader:\n",
    "        y_idx_l.append(batch['y'].numpy())\n",
    "        hat_y_prob = model(batch['X'].to(param_model.device))\n",
    "        hat_y_idx_l.append(np.argmax(hat_y_prob.detach().cpu().numpy(), axis=1))\n",
    "    y_idx_v = np.concatenate(y_idx_l)\n",
    "    hat_y_idx_v = np.concatenate(hat_y_idx_l)\n",
    "    print(y_idx_v.shape)\n",
    "    print(hat_y_idx_v.shape)\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    classification_reports = classification_report(y_true=y_idx_v, y_pred=hat_y_idx_v, output_dict=True, zero_division=0)\n",
    "    pp.pprint(classification_reports['macro avg'])\n",
    "    cm = confusion_matrix(y_true=y_idx_v, y_pred=hat_y_idx_v)\n",
    "    print(cm)\n",
    "    # --- accuracy: np.sum(np.diag(cm))/np.sum(cm)\n",
    "    # --- recall: for c in range(10): print(cm[c,c]/np.sum(cm[c,:]))\n",
    "\n",
    "elif problem=='multilabel':\n",
    "    y_ohe_l = []\n",
    "    hat_y_prob_l = []\n",
    "    for batch in valid_dataloader:\n",
    "        hat_y_prob = F.sigmoid(model(batch['X'].to(param_model.device)))\n",
    "        if problem=='multiclass':       y_ohe_l.append(F.one_hot(batch['y'], 10).numpy())\n",
    "        elif problem=='multilabel':     y_ohe_l.append(batch['y'].numpy())\n",
    "        hat_y_prob_l.append(hat_y_prob.detach().cpu().numpy())\n",
    "    y_ohe_m = np.concatenate(y_ohe_l) # --- convert list to array\n",
    "    hat_y_prob_m = np.concatenate(hat_y_prob_l)\n",
    "\n",
    "    # --- remove classes that do not appear in the validation set\n",
    "    mask = np.sum(y_ohe_m, axis=0)>0\n",
    "    y_ohe_m = y_ohe_m[:,mask]\n",
    "    hat_y_prob_m = hat_y_prob_m[:,mask]\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    auc = roc_auc_score(y_true=y_ohe_m, y_score=hat_y_prob_m, average=\"macro\")\n",
    "    print(f'auc: {auc}')\n",
    "    average_precision = average_precision_score(y_true=y_ohe_m, y_score=hat_y_prob_m, average=\"macro\")\n",
    "    print(f'average_precision: {average_precision}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration: tag-o-gram\n",
    "\n",
    "We illustrate the results obtained by the trained model on a given audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_tag_o_gram(model, audio_v, sr_hz):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # --- Compute the audio features\n",
    "    if do_feature == 'waveform':    feat_value_m = audio_v.reshape(1,-1)\n",
    "    elif do_feature == 'lms':       feat_value_m, _ = feature.f_get_lms(audio_v, sr_hz, param_lms)\n",
    "                \n",
    "    # --- Split matrix into patches\n",
    "    nb_frame = feat_value_m.shape[1]\n",
    "    patch_info_l = feature.f_get_patches(feat_value_m.shape[-1], param_patch.L_frame, param_patch.STEP_frame)\n",
    "    nb_patch = len(patch_info_l)\n",
    "    data_3m = np.zeros((nb_patch, feat_value_m.shape[0], param_patch.L_frame))\n",
    "    for idx in range(nb_patch):\n",
    "        data_3m[idx, :, :] = feat_value_m[:, patch_info_l[idx]['start_frame']:patch_info_l[idx]['end_frame']]\n",
    "\n",
    "    # --- Convert numpy to torch tensor\n",
    "    X = torch.from_numpy(data_3m).float().to(param_model.device)\n",
    "    if do_feature == 'lms': X = X.unsqueeze(1) # --- add channel dimension\n",
    "\n",
    "    # --- Get prediction from model\n",
    "    model.eval()\n",
    "    y_hat = model(X)\n",
    "    if problem=='multiclass':       predicted = F.softmax(y_hat, dim=1)\n",
    "    elif problem=='multilabel':     predicted = F.sigmoid(y_hat)\n",
    "    # --- Convert from torch tensor to numpy\n",
    "    y_hat_m = predicted.cpu().detach().numpy()\n",
    "    print(y_hat_m.shape)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(16, 6))\n",
    "    im = axes.imshow(y_hat_m.T, aspect='auto', interpolation=None, cmap=plt.get_cmap('inferno'))\n",
    "    axes.set_yticks(np.arange(0,len(valid_dataset.labelname_dict_l)))\n",
    "    axes.set_yticklabels(valid_dataset.labelname_dict_l)\n",
    "    axes.grid(True)\n",
    "    fig.colorbar(im, orientation='vertical')\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ---------------------------------------------\n",
    "# ---------------------------------------------\n",
    "audio_file= valid_dataset.audio_file_l[40]\n",
    "\n",
    "with h5py.File(hdf5_audio_file, 'r') as audio_fid:\n",
    "    audio_value_v = audio_fid[audio_file][:]\n",
    "    audio_sr_hz = audio_fid[audio_file].attrs['sr_hz']\n",
    "F_tag_o_gram(model, audio_value_v, audio_sr_hz)\n",
    "plt.title(audio_file)\n",
    "IPython.display.Audio(data=audio_value_v, rate=audio_sr_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display learned filters Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight = model.frontend[1].weight # --- conv1D\n",
    "#weight = model.frontend[1].filters # --- SincNet\n",
    "named_modules_dict = dict(model.named_modules())\n",
    "#weight = named_modules_dict['model.0'][1].weight\n",
    "weight = named_modules_dict['model.0'][1].filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "nb_filter = weight.size()[0]\n",
    "nb_filter = 40\n",
    "for num_filter in range(nb_filter):\n",
    "    value = weight[num_filter,:,:].squeeze().detach().cpu() # --- conv1D\n",
    "    plt.subplot(nb_filter, 2, 2*num_filter+1)\n",
    "    plt.plot( value ); plt.title(num_filter)\n",
    "    plt.subplot(nb_filter, 2, 2*num_filter+2)\n",
    "    plt.plot(np.abs(np.fft.rfft(value)) ); plt.title(num_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display learned filters SincNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_gpeeters_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
